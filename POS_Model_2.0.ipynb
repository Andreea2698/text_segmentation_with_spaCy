{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb8c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab09809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b122b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"This is a sample sentence, from CNN.com. \n",
    "\n",
    "\n",
    "It contains multiple clauses, separated by commas and semicolons; some of which are nested within parentheses.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a6022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences_punct_marks(doc):\n",
    "    # Initialize the list of sentence spans\n",
    "    sentences = []\n",
    "    \n",
    "    # Initialize the start index of each sentence\n",
    "    start = 0\n",
    "    \n",
    "    # Iterate over each token in the Doc object\n",
    "    for i, token in enumerate(doc):\n",
    "        # Check if the token is a punctuation mark that indicates the end of a sentence\n",
    "        if token.is_punct and token.text in [\".\", \"!\", \"?\", \":\", \",\", \";\"]:\n",
    "            # Append the span of text from the start of the sentence to the current token to the list of sentences\n",
    "            sentences.append(doc[start:i+1])\n",
    "            \n",
    "            # Update the start index to the next token\n",
    "            start = i+1\n",
    "    \n",
    "    # Add the last sentence if it's not already included\n",
    "    if start < len(doc):\n",
    "        sentences.append(doc[start:])\n",
    "    \n",
    "    # Return the list of sentence spans\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d23ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_at_cc(sentence): # coordinating conjunction\n",
    "    doc = nlp(sentence)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.dep_ == \"cc\":\n",
    "            chunks.append(doc[start:i])\n",
    "            start = i\n",
    "    chunks.append(doc[start:])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186fa09",
   "metadata": {},
   "source": [
    "The head of a subtree is defined as the root word of a phrase, which has one or more dependents (child words) that modify or complete its meaning. The function checks if the word's dependency label is \"ROOT\" using the token.dep_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bfa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtree_heads(doc):\n",
    "    heads = []\n",
    "    # Iterate through the words in the sentence\n",
    "    for token in doc:\n",
    "        # Check if the word is the head of a subtree\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            # Traverse the subtree to identify phrases\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"nsubj\":\n",
    "                    heads.append(child)\n",
    "                elif child.dep_ == \"prep\":\n",
    "                    heads.append(child)\n",
    "                elif child.dep_ == \"pobj\":\n",
    "                    heads.append(child)\n",
    "                elif child.dep_ == \"amod\":\n",
    "                    heads.append(child)\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a11bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_span_at_heads(tokens, span):\n",
    "    # Convert single token to list\n",
    "    if isinstance(tokens, spacy.tokens.Token):\n",
    "        tokens = [tokens]\n",
    "    # Initialize split points\n",
    "    split_points = [0, len(span)]\n",
    "    for i, token in enumerate(span):\n",
    "        for head in tokens:\n",
    "            if token.text == head.text:\n",
    "                split_points.append(i+1)\n",
    "    # Sort split points and create sub-spans\n",
    "    split_points = sorted(list(set(split_points)))\n",
    "    sub_spans = [span[start:end] for start, end in zip(split_points, split_points[1:])]\n",
    "    return sub_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811c3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_at_end(spans):\n",
    "    cleaned_spans = []\n",
    "    for span in spans:\n",
    "        if span[-1].is_punct:\n",
    "            cleaned_spans.append(span[:-1])\n",
    "        else:\n",
    "            cleaned_spans.append(span)\n",
    "    return cleaned_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b969b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentences_on_new_line(spans):\n",
    "    sentences = []\n",
    "    for span in spans:\n",
    "        sentences.append(span.text)\n",
    "    return '\\n'.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0fe7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the too small segments\n",
    "def merge_too_small(string_):\n",
    "    min_words_perLine = 2\n",
    "    lines = string_.splitlines()  # split the string into lines\n",
    "\n",
    "    # convert lines to a mutable data type\n",
    "    lines_list = list(lines)\n",
    "    \n",
    "    # iterate through each line to determine which lines to merge\n",
    "    for i in range(len(lines_list)-1):\n",
    "        # check if the current line has only one or two words\n",
    "        if len(lines_list[i].split()) <= min_words_perLine:\n",
    "            # if the current  line is the first line (position 0), then merge it with the next one\n",
    "            if i==0:\n",
    "                lines_list[i:i+2] = [' '.join(lines_list[i:i+2])]\n",
    "            if i==len(lines_list)-1:\n",
    "                lines_list[-2:] = [' '.join(lines_list[-2:])]\n",
    "            if i!=0 and i<(len(lines_list)-1):\n",
    "                # check if the previous line has fewer words than the next line\n",
    "                if len(lines_list[i - 1].split()) <= len(lines_list[i+1].split()):\n",
    "                    #add current line to the previous line\n",
    "                    lines_list[i-1:i] = [' '.join(lines_list[i-1:i])]\n",
    "                else:\n",
    "                    #add current line to the next line\n",
    "                    lines_list[i:i+1] = [' '.join(lines_list[i:i+1])]\n",
    "    # join the lines back into a string\n",
    "    merged_string = '\\n'.join(lines_list)\n",
    "    return merged_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f18cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_text(text):\n",
    "    text_ = re.sub(r\"\\s+\", \" \", text)\n",
    "    doc = nlp(text_)\n",
    "    heads = get_subtree_heads(doc)\n",
    "    sentences = list(doc.sents)\n",
    "    smaller_parts = []\n",
    "    smaller_parts_ = []\n",
    "    smaller_parts__ = []\n",
    "    final_list = []\n",
    "    for sentence in sentences:\n",
    "        smaller_parts.append(split_sentences_punct_marks(sentence))\n",
    "    for x in smaller_parts:\n",
    "        for xx in x:\n",
    "            smaller_parts_.append(split_at_cc(xx.text))\n",
    "    for x in smaller_parts_:\n",
    "        for xx in x:\n",
    "            smaller_parts__.append(split_span_at_heads(heads, xx))\n",
    "    for x in smaller_parts__:\n",
    "        for xx in x: \n",
    "            final_list.append(xx)\n",
    "    cleaned_spans = remove_punctuation_at_end(final_list)\n",
    "    sentences_on_new_line = join_sentences_on_new_line(cleaned_spans)\n",
    "    return merge_too_small(sentences_on_new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4392f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = break_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de8d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence\n",
      "from CNN.com\n",
      "It contains multiple clauses\n",
      "separated by commas\n",
      "and semicolons\n",
      "some of which are nested within parentheses\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef66199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazyKh_39",
   "language": "python",
   "name": "lazykh_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
